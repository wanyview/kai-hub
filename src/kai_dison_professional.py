#!/usr/bin/env python3
"""
KaiDison ä¸“ä¸šçº§æ•°å­—ç§‘å­¦å®¶ (v2.0)
è·¨åŸŸå…³è”å¼•æ“ + çªç ´æ£€æµ‹ + å…±è¯†è¿½è¸ª + è¶‹åŠ¿é¢„æµ‹
æ”¯æŒ: ä¸ªäºº + å›¢é˜Ÿ + ç»„ç»‡

æ ¸å¿ƒç†å¿µ: AIç§‘å­¦å®¶ = ä¸ªäºº + å›¢é˜Ÿ + ç»„ç»‡
"""

import json
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from collections import defaultdict
import urllib.request


@dataclass
class Capsule:
    """çŸ¥è¯†èƒ¶å›Š"""
    title: str
    domain: str
    topics: List[str]
    insight: str
    evidence: List[str]
    authors: List[str]
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())


# ============ v2.0 æ–°å¢: å¤šç±»å‹ AI ç§‘å­¦å®¶ ============

@dataclass
class IndividualScientist:
    """ä¸ªäººç§‘å­¦å®¶"""
    id: str = field(default_factory=lambda: f"ind_{datetime.now().strftime('%Y%m%d%H%M%S')}")
    name: str = ""
    name_en: str = ""
    type: str = "individual"
    
    # åŸºæœ¬ä¿¡æ¯
    field: str = ""
    era: str = ""
    nationality: str = ""
    
    # å­¦æœ¯è´¡çŒ®
    contributions: List[str] = field(default_factory=list)
    publications: List[str] = field(default_factory=list)
    awards: List[str] = field(default_factory=list)
    
    # æ€æƒ³é£æ ¼
    thinking_style: str = ""
    personality: str = ""
    famous_quotes: List[str] = field(default_factory=list)
    
    # å…³è”
    mentors: List[str] = field(default_factory=list)
    collaborators: List[str] = field(default_factory=list)
    followers: List[str] = field(default_factory=list)


@dataclass
class ResearchTeam:
    """ç ”ç©¶å›¢é˜Ÿ"""
    id: str = field(default_factory=lambda: f"team_{datetime.now().strftime('%Y%m%d%H%M%S')}")
    name: str = ""
    type: str = "team"
    
    # åŸºæœ¬ä¿¡æ¯
    organization: str = ""
    field: str = ""
    founded: str = ""
    
    # æˆå‘˜
    leader: str = ""
    members: List[str] = field(default_factory=list)
    alumni: List[str] = field(default_factory=list)
    
    # äº§å‡º
    projects: List[str] = field(default_factory=list)
    publications: List[str] = field(default_factory=list)
    products: List[str] = field(default_factory=list)
    
    # åä½œé£æ ¼
    collaboration_style: str = ""
    culture: str = ""
    
    # æ–¹æ³•è®º
    methodology: List[str] = field(default_factory=list)
    tools: List[str] = field(default_factory=list)


@dataclass
class ResearchOrganization:
    """ç ”ç©¶ç»„ç»‡"""
    id: str = field(default_factory=lambda: f"org_{datetime.now().strftime('%Y%m%d%H%M%S')}")
    name: str = ""
    type: str = "organization"
    
    # åŸºæœ¬ä¿¡æ¯
    field: str = ""
    founded: str = ""
    location: str = ""
    
    # ç»“æ„
    structure: str = ""
    scale: str = ""
    
    # é¢†å¯¼å±‚
    founders: List[str] = field(default_factory=list)
    leadership: List[str] = field(default_factory=list)
    
    # å†å²
    milestones: List[str] = field(default_factory=list)
    spin_offs: List[str] = field(default_factory=list)
    
    # äº§å‡º
    labs: List[str] = field(default_factory=list)
    research_areas: List[str] = field(default_factory=list)
    
    # æ–‡åŒ–
    mission: str = ""
    values: List[str] = field(default_factory=list)
    culture: str = ""
    research_philosophy: str = ""


@dataclass
class Association:
    """è·¨åŸŸå…³è”"""
    domain_a: str
    domain_b: str
    strength: float  # 0-1
    topics: List[str]
    description: str
    recommendation: str


@dataclass
class Breakthrough:
    """æŠ€æœ¯çªç ´"""
    title: str
    domains: List[str]
    significance: float  # 0-100
    evidence: List[str]
    type: str  # method/data/concept
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())


@dataclass
class Consensus:
    """å…±è¯†è¾¾æˆ"""
    topic: str
    domains: List[str]
    strength: float  # 0-1
    positions: Dict[str, str]  # domain -> position
    evolution: List[Dict]  # è§‚ç‚¹æ¼”åŒ–å†å²
    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())


class CrossDomainEngine:
    """è·¨åŸŸå…³è”å¼•æ“"""
    
    # é¢†åŸŸå…³é”®è¯æ˜ å°„ï¼ˆè‹±æ–‡å…³é”®è¯ï¼‰
    DOMAIN_KEYWORDS = {
        'neuroscience': ['neural', 'brain', 'cortex', 'neuron', 'synaptic', 'motor', 'sensory', 'neuroplasticity', 'brain'],
        'ai': ['ai', 'ml', 'deep learning', 'algorithm', 'neural network', 'model', 'decoding', 'end-to-end', 'personalized'],
        'ethics': ['ethics', 'privacy', 'fairness', 'rights', 'enhancement', 'privacy', 'access'],
        'materials': ['material', 'electrode', 'flexible', 'biocompatible', 'nano', 'polymer', 'conductive'],
        'medical': ['clinical', 'rehabilitation', 'therapy', 'patient', 'medical', 'treatment'],
        'physics': ['gravity', 'physics', 'force', 'quantum', 'mechanics'],
        'technology': ['technology', 'invention', 'device', 'innovation'],
        'climate': ['climate', 'environment', 'temperature', 'agriculture'],
        'biotech': ['synthetic biology', 'biology', 'genetic', 'bio']
    }
    
    # ä¸­æ–‡å…³é”®è¯
    DOMAIN_KEYWORDS_CN = {
        'neuroscience': ['ç¥ç»', 'å¤§è„‘', 'çš®å±‚', 'çªè§¦', 'è¿åŠ¨çš®å±‚', 'æ„Ÿè§‰åé¦ˆ', 'ç¥ç»å¯å¡‘æ€§'],
        'ai': ['AI', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'è§£ç ', 'ç®—æ³•', 'ç¥ç»ç½‘ç»œ', 'ç«¯åˆ°ç«¯', 'ä¸ªæ€§åŒ–'],
        'ethics': ['ä¼¦ç†', 'éšç§', 'å…¬å¹³', 'æƒåˆ©', 'å¢å¼º', 'è¾¹ç•Œ'],
        'materials': ['ææ–™', 'ç”µæ', 'æŸ”æ€§', 'ç”Ÿç‰©ç›¸å®¹', 'çº³ç±³', 'èšåˆç‰©', 'å¯¼ç”µ'],
        'medical': ['ä¸´åºŠ', 'åº·å¤', 'æ²»ç–—', 'æ‚£è€…', 'åŒ»ç–—', 'è¿åŠ¨éšœç¢'],
        'physics': ['é‡åŠ›', 'ç‰©ç†', 'åŠ›å­¦', 'é‡å­'],
        'technology': ['æŠ€æœ¯', 'å‘æ˜', 'åˆ›æ–°', 'è®¾å¤‡'],
        'biotech': ['åˆæˆç”Ÿç‰©', 'ç”Ÿç‰©', 'é—ä¼ ']
    }
    
    def __init__(self):
        self.associations: List[Association] = []
    
    def extract_keywords(self, text: str) -> List[Tuple[str, str]]:
        """æå–æ–‡æœ¬å…³é”®è¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
        keywords = []
        text_lower = text.lower()
        
        for domain, words in self.DOMAIN_KEYWORDS.items():
            for word in words:
                if word.lower() in text_lower:
                    keywords.append((domain, word))
        
        for domain, words in self.DOMAIN_KEYWORDS_CN.items():
            for word in words:
                if word in text:
                    keywords.append((domain, word))
        
        return keywords
    
    def find_associations(self, capsules: List[Capsule]) -> List[Association]:
        """å‘ç°è·¨åŸŸå…³è”"""
        associations = []
        
        # æŒ‰é¢†åŸŸåˆ†ç»„ï¼ˆæ ‡å‡†åŒ–é¢†åŸŸåç§°ï¼‰
        by_domain = defaultdict(list)
        for capsule in capsules:
            domain = capsule.domain.lower().strip()
            # æ ‡å‡†åŒ–é¢†åŸŸåç§°
            if domain in ['neuroscience', 'ç¥ç»ç§‘å­¦']:
                domain = 'neuroscience'
            elif domain in ['ai', 'äººå·¥æ™ºèƒ½']:
                domain = 'ai'
            elif domain in ['ethics', 'ä¼¦ç†']:
                domain = 'ethics'
            elif domain in ['materials', 'ææ–™ç§‘å­¦']:
                domain = 'materials'
            elif domain in ['medical', 'åŒ»å­¦']:
                domain = 'medical'
            elif domain in ['physics', 'ç‰©ç†']:
                domain = 'physics'
            elif domain in ['technology', 'æŠ€æœ¯']:
                domain = 'technology'
            elif domain in ['biotech', 'åˆæˆç”Ÿç‰©']:
                domain = 'biotech'
            else:
                domain = domain  # ä¿æŒåŸæ ·
            by_domain[domain].append(capsule)
        
        # æ£€æŸ¥æ‰€æœ‰é¢†åŸŸå¯¹
        domains = list(by_domain.keys())
        
        for i, d1 in enumerate(domains):
            for d2 in domains[i+1:]:
                # è®¡ç®—å…³è”å¼ºåº¦
                strength = self._calculate_association_strength(
                    by_domain[d1], by_domain[d2]
                )
                
                if strength > 0.1:  # é™ä½é˜ˆå€¼ä»¥æ£€æµ‹æ›´å¤šå…³è”
                    topics = self._find_common_topics(
                        by_domain[d1], by_domain[d2]
                    )
                    
                    assoc = Association(
                        domain_a=d1,
                        domain_b=d2,
                        strength=strength,
                        topics=topics,
                        description=f"{d1} ä¸ {d2} åœ¨ä»¥ä¸‹æ–¹é¢å­˜åœ¨å…³è”",
                        recommendation=self._generate_recommendation(d1, d2, topics)
                    )
                    associations.append(assoc)
        
        # æŒ‰å¼ºåº¦æ’åº
        associations.sort(key=lambda x: -x.strength)
        self.associations = associations[:10]  # æœ€å¤š10ä¸ª
        return self.associations
    
    def _calculate_association_strength(self, caps1: List[Capsule], caps2: List[Capsule]) -> float:
        """è®¡ç®—ä¸¤ä¸ªé¢†åŸŸä¹‹é—´çš„å…³è”å¼ºåº¦"""
        if not caps1 or not caps2:
            return 0.0
        
        # æå–æ‰€æœ‰å…³é”®è¯
        keywords1 = set()
        keywords2 = set()
        
        for cap in caps1:
            kw = self.extract_keywords(cap.insight + ' ' + ' '.join(cap.topics))
            keywords1.update([w for _, w in kw])
        
        for cap in caps2:
            kw = self.extract_keywords(cap.insight + ' ' + ' '.join(cap.topics))
            keywords2.update([w for _, w in kw])
        
        # Jaccard ç›¸ä¼¼åº¦
        if not keywords1 or not keywords2:
            return 0.0
        
        intersection = keywords1 & keywords2
        union = keywords1 | keywords2
        
        return len(intersection) / len(union) if union else 0.0
    
    def _find_common_topics(self, caps1: List[Capsule], caps2: List[Capsule]) -> List[str]:
        """æŸ¥æ‰¾å…±åŒè¯é¢˜"""
        topics = set()
        for cap in caps1 + caps2:
            for t in cap.topics:
                topics.add(t)
        return list(topics)[:5]
    
    def _generate_recommendation(self, d1: str, d2: str, topics: List[str]) -> str:
        """ç”Ÿæˆå»ºè®®"""
        return f"å»ºè®®ç»„ç»‡ {d1} ä¸ {d2} é¢†åŸŸçš„è”åˆè®¨è®ºä¼šï¼Œé‡ç‚¹æ¢è®¨: {', '.join(topics[:3])}"


class BreakthroughDetector:
    """çªç ´æ£€æµ‹ç®—æ³•"""
    
    # çªç ´ç‰¹å¾è¯
    BREAKTHROUGH_PATTERNS = {
        'method': [r'æ–°æ–¹æ³•', r'çªç ´', r'åˆ›æ–°', r'é¦–æ¬¡', r'ç«¯åˆ°ç«¯', r'é©å‘½æ€§'],
        'data': [r'å¤§è§„æ¨¡', r'æ–°æ•°æ®', r'æ•°æ®é›†', r'é«˜åˆ†è¾¨ç‡', r'å®æ—¶'],
        'concept': [r'æ–°èŒƒå¼', r'ç†è®º', r'æ¦‚å¿µ', r'æ¡†æ¶', r'æ¨¡å‹é‡æ„']
    }
    
    def __init__(self):
        self.breakthroughs: List[Breakthrough] = []
    
    def detect(self, capsules: List[Capsule]) -> List[Breakthrough]:
        """æ£€æµ‹æŠ€æœ¯çªç ´"""
        breakthroughs = []
        
        for capsule in capsules:
            # åˆ†ææ¯ä¸ªèƒ¶å›Š
            text = capsule.insight + ' ' + ' '.join(capsule.topics)
            
            # æ£€æµ‹çªç ´ç±»å‹
            btype = self._detect_type(text)
            significance = self._calculate_significance(text, capsule.evidence)
            
            if significance > 60:  # é˜ˆå€¼
                bt = Breakthrough(
                    title=capsule.title,
                    domains=[capsule.domain],
                    significance=significance,
                    evidence=capsule.evidence,
                    type=btype
                )
                breakthroughs.append(bt)
        
        self.breakthroughs = breakthroughs
        return breakthroughs
    
    def _detect_type(self, text: str) -> str:
        """æ£€æµ‹çªç ´ç±»å‹"""
        for btype, patterns in self.BREAKTHROUGH_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return btype
        return 'concept'  # é»˜è®¤
    
    def _calculate_significance(self, text: str, evidence: List[str]) -> float:
        """è®¡ç®—çªç ´é‡è¦æ€§"""
        score = 0.0
        
        # ç‰¹å¾è¯åŒ¹é…
        for patterns in self.BREAKTHROUGH_PATTERNS.values():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    score += 15
        
        # è¯æ®æ•°é‡
        score += min(len(evidence) * 10, 30)
        
        # é¢†åŸŸäº¤å‰
        if len(set(text.split()) & {'è·¨å­¦ç§‘', 'èåˆ', 'ç»“åˆ', 'é›†æˆ'}):
            score += 20
        
        return min(score, 100)


class ConsensusTracker:
    """å…±è¯†è¾¾æˆè¿½è¸ª"""
    
    def __init__(self):
        self.consensuses: List[Consensus] = []
        self.positions_history: Dict[str, List[Dict]] = defaultdict(list)
    
    def track(self, capsules: List[Capsule]) -> List[Consensus]:
        """è¿½è¸ªå…±è¯†è¾¾æˆ"""
        # æŒ‰è¯é¢˜åˆ†ç»„
        by_topic = defaultdict(list)
        for capsule in capsules:
            for topic in capsule.topics:
                by_topic[topic].append(capsule)
        
        # åˆ†ææ¯ä¸ªè¯é¢˜
        for topic, caps in by_topic.items():
            if len(caps) >= 2:  # è‡³å°‘ä¸¤ä¸ªèƒ¶å›Šè®¨è®ºåŒä¸€è¯é¢˜
                consensus = self._analyze_consensus(topic, caps)
                if consensus:
                    self.consensuses.append(consensus)
        
        return self.consensuses
    
    def _analyze_consensus(self, topic: str, capsules: List[Capsule]) -> Optional[Consensus]:
        """åˆ†æå…±è¯†"""
        # æ”¶é›†å„æ–¹è§‚ç‚¹
        positions = {}
        for cap in capsules:
            positions[cap.domain] = cap.insight[:200]
        
        # è®¡ç®—ä¸€è‡´æ€§
        strength = self._calculate_agreement(positions)
        
        if strength > 0.6:  # é˜ˆå€¼
            return Consensus(
                topic=topic,
                domains=list(positions.keys()),
                strength=strength,
                positions=positions,
                evolution=self.positions_history.get(topic, [])
            )
        return None
    
    def _calculate_agreement(self, positions: Dict[str, str]) -> float:
        """è®¡ç®—è§‚ç‚¹ä¸€è‡´æ€§"""
        if len(positions) < 2:
            return 0.0
        
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        texts = list(positions.values())
        
        # æ£€æŸ¥å…³é”®è¯é‡å 
        keywords = [set(t.split()) for t in texts]
        intersection = keywords[0]
        for k in keywords[1:]:
            intersection = intersection & k
        
        union = keywords[0]
        for k in keywords[1:]:
            union = union | k
        
        return len(intersection) / len(union) if union else 0.0


class TrendPredictor:
    """è¶‹åŠ¿é¢„æµ‹"""
    
    def __init__(self):
        self.trends: List[Dict] = []
    
    def predict(self, capsules: List[Capsule]) -> List[Dict]:
        """é¢„æµ‹è¶‹åŠ¿"""
        # ç»Ÿè®¡è¯é¢˜é¢‘ç‡
        topic_freq = defaultdict(int)
        domain_freq = defaultdict(int)
        
        for capsule in capsules:
            for topic in capsule.topics:
                topic_freq[topic] += 1
            domain_freq[capsule.domain] += 1
        
        # è¯†åˆ«ä¸Šå‡è¶‹åŠ¿
        rising = []
        for topic, freq in sorted(topic_freq.items(), key=lambda x: -x[1])[:5]:
            rising.append({
                'topic': topic,
                'frequency': freq,
                'trend': 'rising' if freq > 3 else 'stable'
            })
        
        # è¯†åˆ«çƒ­é—¨é¢†åŸŸ
        hot_domains = []
        for domain, freq in sorted(domain_freq.items(), key=lambda x: -x[1]):
            hot_domains.append({
                'domain': domain,
                'count': freq,
                'growth': 'high' if freq > 3 else 'medium'
            })
        
        self.trends = {
            'rising_topics': rising,
            'hot_domains': hot_domains,
            'prediction_period': 'next_3_months'
        }
        
        return [self.trends]


class KaiDisonProfessional:
    """KaiDison ä¸“ä¸šçº§æ•°å­—ç§‘å­¦å®¶"""
    
    def __init__(self, capsulehub_url: str = "http://localhost:8001"):
        self.name = "KaiDison"
        self.level = "Professional (L5)"
        self.capsulehub_url = capsulehub_url
        
        # æ ¸å¿ƒå¼•æ“
        self.cross_domain = CrossDomainEngine()
        self.breakthrough_detector = BreakthroughDetector()
        self.consensus_tracker = ConsensusTracker()
        self.trend_predictor = TrendPredictor()
        
        # çŠ¶æ€
        self.status = "active"
        self.last_scan = None
        self.stats = {
            "cross_domain_links": 0,
            "fusion_capsules": 0,
            "breakthroughs": 0,
            "consensus": 0
        }
    
    def scan_and_analyze(self) -> Dict:
        """æ‰«æå¹¶åˆ†ææ‰€æœ‰èƒ¶å›Š"""
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ KaiDison ä¸“ä¸šåˆ†ææŠ¥å‘Š")
        print(f"{'='*60}\n")
        
        # 1. è·å–èƒ¶å›Š
        print("ğŸ“¥ æ­£åœ¨è·å–çŸ¥è¯†èƒ¶å›Š...")
        capsules = self._fetch_capsules()
        print(f"   è·å–åˆ° {len(capsules)} ä¸ªèƒ¶å›Š\n")
        
        # 2. è·¨åŸŸå…³è”
        print("ğŸ”— è·¨åŸŸå…³è”åˆ†æ...")
        associations = self.cross_domain.find_associations(capsules)
        self.stats["cross_domain_links"] = len(associations)
        for assoc in associations:
            print(f"   â€¢ {assoc.domain_a} â†” {assoc.domain_b}: {assoc.strength:.1%}")
        print()
        
        # 3. çªç ´æ£€æµ‹
        print("ğŸ’¥ æŠ€æœ¯çªç ´æ£€æµ‹...")
        breakthroughs = self.breakthrough_detector.detect(capsules)
        self.stats["breakthroughs"] = len(breakthroughs)
        for bt in breakthroughs:
            print(f"   â€¢ {bt.title[:35]}: {bt.significance:.0f}% ({bt.type})")
        print()
        
        # 4. å…±è¯†è¿½è¸ª
        print("ğŸ¤ å…±è¯†è¾¾æˆè¿½è¸ª...")
        consensuses = self.consensus_tracker.track(capsules)
        self.stats["consensus"] = len(consensuses)
        for cs in consensuses:
            print(f"   â€¢ {cs.topic}: {cs.strength:.1%} ({len(cs.domains)}æ–¹)")
        print()
        
        # 5. è¶‹åŠ¿é¢„æµ‹
        print("ğŸ”® è¶‹åŠ¿é¢„æµ‹...")
        trends = self.trend_predictor.predict(capsules)
        if trends:
            t = trends[0]
            print(f"   ä¸Šå‡è¯é¢˜: {', '.join([x['topic'] for x in t.get('rising_topics', [])[:3]])}")
            print(f"   çƒ­é—¨é¢†åŸŸ: {', '.join([x['domain'] for x in t.get('hot_domains', [])[:3]])}")
        print()
        
        # 6. ç”Ÿæˆèåˆèƒ¶å›Š
        print("ğŸ§¬ ç”Ÿæˆèåˆèƒ¶å›Š...")
        fusion_capsules = self._generate_fusion_capsules(associations, breakthroughs, capsules)
        self.stats["fusion_capsules"] = len(fusion_capsules)
        for fc in fusion_capsules:
            print(f"   â€¢ {fc.title[:40]}")
        print()
        
        self.last_scan = datetime.utcnow().isoformat()
        
        return {
            "status": "success",
            "kaiDison": {
                "name": self.name,
                "level": self.level,
                "status": self.status,
                "last_scan": self.last_scan
            },
            "stats": self.stats,
            "associations": [
                {
                    "domains": [a.domain_a, a.domain_b],
                    "strength": a.strength,
                    "topics": a.topics,
                    "recommendation": a.recommendation
                }
                for a in associations
            ],
            "breakthroughs": [
                {
                    "title": b.title,
                    "significance": b.significance,
                    "type": b.type
                }
                for b in breakthroughs
            ],
            "consensus": [
                {
                    "topic": c.topic,
                    "domains": c.domains,
                    "strength": c.strength
                }
                for c in consensuses
            ],
            "trends": trends,
            "fusion_capsules": [
                {
                    "title": fc.title,
                    "domain": fc.domain,
                    "topics": fc.topics,
                    "insight": fc.insight
                }
                for fc in fusion_capsules
            ]
        }
    
    def _fetch_capsules(self) -> List[Capsule]:
        """ä» CapsuleHub è·å–èƒ¶å›Š"""
        capsules = []
        
        try:
            url = f"{self.capsulehub_url}/api/capsules?limit=100"
            with urllib.request.urlopen(url, timeout=5) as response:
                data = json.loads(response.read().decode())
                
                for c in data.get("capsules", []):
                    capsules.append(Capsule(
                        title=c.get("title", ""),
                        domain=c.get("domain", ""),
                        topics=c.get("topics", []),
                        insight=c.get("insight", ""),
                        evidence=c.get("evidence", []),
                        authors=c.get("authors", [])
                    ))
        except Exception as e:
            print(f"   âš ï¸ è·å–èƒ¶å›Šå¤±è´¥: {e}")
        
        return capsules
    
    def _generate_fusion_capsules(self, associations: List[Association], 
                                  breakthroughs: List[Breakthrough],
                                  capsules: List[Capsule]) -> List[Capsule]:
        """ç”Ÿæˆè·¨åŸŸèåˆèƒ¶å›Š"""
        fusion_capsules = []
        
        # åŸºäºå¼ºå…³è”ç”Ÿæˆèåˆèƒ¶å›Šï¼ˆé™ä½é˜ˆå€¼ï¼‰
        for assoc in associations:
            if assoc.strength > 0.05:  # é™ä½åˆ°5%
                fc = Capsule(
                    title=f"è·¨åŸŸèåˆ: {assoc.domain_a} + {assoc.domain_b}",
                    domain=f"{assoc.domain_a}+{assoc.domain_b}",
                    topics=assoc.topics[:3] if assoc.topics else ['è·¨åŸŸèåˆ'],
                    insight=f"é€šè¿‡è·¨åŸŸå…³è”åˆ†æå‘ç°ï¼Œ{assoc.domain_a} ä¸ {assoc.domain_b} å­˜åœ¨ {assoc.strength:.1%} çš„å…³è”å¼ºåº¦ã€‚å»ºè®®ä¸¤ä¸ªé¢†åŸŸçš„ä¸“å®¶è¿›è¡Œè”åˆè®¨è®ºï¼Œä»¥ä¿ƒè¿›çŸ¥è¯†èåˆå’Œåˆ›æ–°çªç ´ã€‚",
                    evidence=[f"å…³è”å¼ºåº¦: {assoc.strength:.1%}", assoc.recommendation],
                    authors=["KaiDison"]
                )
                fusion_capsules.append(fc)
        
        # åŸºäºçªç ´ç”Ÿæˆèåˆèƒ¶å›Š
        for bt in breakthroughs:
            if bt.significance > 50:  # é™ä½åˆ°50%
                fc = Capsule(
                    title=f"çªç ´èåˆ: {bt.title}",
                    domain="+".join(bt.domains),
                    topics=bt.domains,
                    insight=f"æŠ€æœ¯çªç ´æ£€æµ‹å‘ç°: {bt.title} å…·æœ‰ {bt.significance:.0f}% çš„é‡è¦æ€§è¯„åˆ†ã€‚ç±»å‹: {bt.type}ã€‚å»ºè®®ç›¸å…³é¢†åŸŸé‡ç‚¹å…³æ³¨å¹¶è·Ÿè¿›ç ”ç©¶ã€‚",
                    evidence=bt.evidence,
                    authors=["KaiDison"]
                )
                fusion_capsules.append(fc)
        
        return fusion_capsules[:10]  # æœ€å¤š10ä¸ª
    
    def get_status(self) -> Dict:
        """è·å–çŠ¶æ€"""
        return {
            "name": self.name,
            "level": self.level,
            "status": self.status,
            "last_scan": self.last_scan,
            "stats": self.stats
        }


def main():
    """ä¸»å‡½æ•° - è¿è¡Œ KaiDison åˆ†æ"""
    kaiDison = KaiDisonProfessional()
    
    # è¿è¡Œåˆ†æ
    result = kaiDison.scan_and_analyze()
    
    # è¾“å‡ºæŠ¥å‘Š
    print(f"\n{'='*60}")
    print(f"ğŸ“Š KaiDison åˆ†æå®Œæˆ")
    print(f"{'='*60}")
    print(f"\nğŸ”— è·¨åŸŸå…³è”: {result['stats']['cross_domain_links']}")
    print(f"ğŸ’¥ æŠ€æœ¯çªç ´: {result['stats']['breakthroughs']}")
    print(f"ğŸ¤ å…±è¯†è¾¾æˆ: {result['stats']['consensus']}")
    print(f"ğŸ§¬ èåˆèƒ¶å›Š: {result['stats']['fusion_capsules']}")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "/Users/wanyview/clawd/kai-hub/reports/kai_dison_report.json"
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    return result


if __name__ == "__main__":
    import os
    main()


# ============ v2.0 æ–°å¢: å¤šç±»å‹ AI ç§‘å­¦å®¶é¢„è®¾ ============

# å›¢é˜Ÿé¢„è®¾
TEAM_PRESETS = {
    "OpenAI Safety Team": {
        "name": "OpenAI å®‰å…¨å›¢é˜Ÿ",
        "organization": "OpenAI",
        "field": "AI Safety",
        "founded": "2020",
        "leader": "Ilya Sutskever",
        "members": ["Dario Amodei", "Jan Leike", "Paul Christiano"],
        "projects": ["GPT-4 Alignment", "Constitutional AI", "RLHF"],
        "collaboration_style": "è·¨å­¦ç§‘åä½œ",
        "culture": "å®‰å…¨ä¼˜å…ˆ",
        "methodology": ["RLHF", "Constitutional AI", "Scalable Oversight"]
    },
    "Apple Vision Pro Team": {
        "name": "Apple Vision Pro å›¢é˜Ÿ",
        "organization": "Apple",
        "field": "AR/VR",
        "founded": "2018",
        "leader": "Mike Rockwell",
        "members": ["John Ternus", "Alan Dye"],
        "products": ["Apple Vision Pro", "visionOS"],
        "collaboration_style": "ä¿å¯†åä½œ",
        "culture": "å®Œç¾ä¸»ä¹‰",
        "methodology": ["äººæœºäº¤äº’", "æ˜¾ç¤ºæŠ€æœ¯", "ä¼ æ„Ÿå™¨èåˆ"]
    },
    "DeepMind AlphaFold": {
        "name": "DeepMind AlphaFold å›¢é˜Ÿ",
        "organization": "DeepMind",
        "field": "Computational Biology",
        "founded": "2016",
        "leader": "Demis Hassabis",
        "members": ["John Jumper", "David Baker"],
        "projects": ["AlphaFold 1", "AlphaFold 2", "AlphaFold 3"],
        "collaboration_style": "AI + ç”Ÿç‰©å­¦å®¶",
        "culture": "ç§‘å­¦çªç ´",
        "methodology": ["æ·±åº¦å­¦ä¹ ", "è›‹ç™½è´¨ç»“æ„é¢„æµ‹", "ç”Ÿç‰©ä¿¡æ¯å­¦"]
    }
}

# ç»„ç»‡é¢„è®¾
ORG_PRESETS = {
    "Harvard University": {
        "name": "å“ˆä½›å¤§å­¦",
        "field": "ç»¼åˆæ€§å¤§å­¦",
        "founded": "1636",
        "location": "é©¬è¨è¯¸å¡å·å‰‘æ¡¥å¸‚",
        "structure": "å­¦é™¢åˆ¶",
        "scale": "20000+ å­¦ç”Ÿ",
        "founders": ["John Harvard"],
        "leadership": ["Claudine Gay"],
        "milestones": ["å»ºç«‹ç¾å›½ç¬¬ä¸€æ‰€å¤§å­¦", "æ‹‰å¾·å…‹åˆ©å¤«å­¦é™¢åˆå¹¶"],
        "labs": ["å“ˆä½›åŒ»å­¦é™¢", "å“ˆä½›æ³•å­¦é™¢", "å“ˆä½›å•†å­¦é™¢"],
        "research_areas": ["åŒ»å­¦", "æ³•å­¦", "ç»æµå­¦", "è®¡ç®—æœºç§‘å­¦"],
        "mission": "è¿½æ±‚çœŸç†ï¼ŒæœåŠ¡ç¤¾ä¼š",
        "culture": "å­¦æœ¯è‡ªç”±ï¼Œç²¾è‹±æ•™è‚²",
        "research_philosophy": "åŸºç¡€ç ”ç©¶ä¸åº”ç”¨å¹¶é‡"
    },
    "Bell Labs": {
        "name": "è´å°”å®éªŒå®¤",
        "field": "é€šä¿¡ä¸è®¡ç®—",
        "founded": "1925",
        "location": "æ–°æ³½è¥¿å·é»˜é‡Œå±±",
        "structure": "ä¼ä¸šç ”ç©¶é™¢",
        "scale": "3000+ ç ”ç©¶å‘˜",
        "founders": ["AT&T", "Western Electric"],
        "milestones": ["æ™¶ä½“ç®¡å‘æ˜", "æ¿€å…‰ç†è®º", "Unixæ“ä½œç³»ç»Ÿ", "Cè¯­è¨€"],
        "spin_offs": ["æœ—è®¯ç§‘æŠ€"],
        "labs": ["è®¡ç®—ç§‘å­¦ä¸­å¿ƒ", "ç‰©ç†å®éªŒå®¤"],
        "research_areas": ["é€šä¿¡", "è®¡ç®—æœº", "ç‰©ç†", "ææ–™"],
        "mission": "å‘æ˜ä¿ƒè¿›äººç±»è¿›æ­¥çš„æŠ€æœ¯",
        "culture": "åŸºç¡€ç ”ç©¶å¯¼å‘ï¼Œé•¿æœŸæŠ•å…¥",
        "research_philosophy": "å¥½å¥‡å¿ƒé©±åŠ¨ï¼Œè‡ªç”±æ¢ç´¢"
    },
    "MIT Media Lab": {
        "name": "MIT åª’ä½“å®éªŒå®¤",
        "field": "åª’ä½“æŠ€æœ¯ä¸è®¾è®¡",
        "founded": "1985",
        "location": "é©¬è¨è¯¸å¡å·å‰‘æ¡¥å¸‚",
        "structure": "è·¨å­¦ç§‘å®éªŒå®¤",
        "scale": "500+ æˆå‘˜",
        "founders": ["Nicholas Negroponte", "Jerome Wiesner"],
        "leadership": ["Dava Newman"],
        "milestones": ["ç”µå­å¢¨æ°´", "å¯ç©¿æˆ´è®¡ç®—", "One Laptop per Child"],
        "research_areas": ["äººæœºäº¤äº’", "äººå·¥æ™ºèƒ½", "è®¾è®¡", "ç”Ÿç‰©å·¥ç¨‹"],
        "mission": "åˆ›é€ æœ‰å‰æ™¯çš„çªç ´æ€§æŠ€æœ¯",
        "culture": "åä¼ ç»Ÿï¼Œè·¨å­¦ç§‘ï¼Œå¼€æ”¾",
        "research_philosophy": "åå­¦ç§‘ï¼Œé€†å‘è®¾è®¡"
    },
    "DeepMind": {
        "name": "DeepMind",
        "field": "äººå·¥æ™ºèƒ½",
        "founded": "2010",
        "location": "ä¼¦æ•¦",
        "structure": "ä¼ä¸šç ”ç©¶é™¢",
        "scale": "1000+ å‘˜å·¥",
        "founders": ["Demis Hassabis", "Mustafa Suleyman", "Shane Legg"],
        "leadership": ["Demis Hassabis (CEO)"],
        "milestones": ["AlphaGo", "AlphaFold", "AlphaCode"],
        "spin_offs": ["Isomorphic Labs"],
        "research_areas": ["æ·±åº¦å­¦ä¹ ", "å¼ºåŒ–å­¦ä¹ ", "è›‹ç™½è´¨ç»“æ„", "æ¸¸æˆAI"],
        "mission": "è§£å†³æ™ºèƒ½é—®é¢˜ï¼Œæ¨åŠ¨ç§‘å­¦çªç ´",
        "culture": "ç§‘å­¦ä¼˜å…ˆï¼Œå¼€æ”¾ç ”ç©¶",
        "research_philosophy": "é€šç”¨äººå·¥æ™ºèƒ½"
    }
}


# ============ v2.0 æ–°å¢: æ‰©å±•åˆ†ææ–¹æ³• ============

def analyze_entity_interaction(
    entity_a: Dict,
    entity_b: Dict,
    context: str = ""
) -> Dict:
    """åˆ†æä¸¤ä¸ªå®ä½“çš„äº¤äº’"""
    
    type_a = entity_a.get('type', 'individual')
    type_b = entity_b.get('type', 'individual')
    
    # åˆ†æç±»å‹ç»„åˆ
    analysis = {
        "entity_a": entity_a,
        "entity_b": entity_b,
        "type_pair": (type_a, type_b),
        "context": context,
        "insights": []
    }
    
    # ä¸ªäºº vs ä¸ªäºº
    if type_a == 'individual' and type_b == 'individual':
        analysis["insights"].append("ä¸¤ä½ä¸ªäººç§‘å­¦å®¶çš„æ€æƒ³ç¢°æ’")
        if entity_a.get('field') != entity_b.get('field'):
            analysis["insights"].append(f"è·¨é¢†åŸŸå…³è”: {entity_a['field']} â†” {entity_b['field']}")
    
    # å›¢é˜Ÿ vs ç»„ç»‡
    elif type_a == 'team' and type_b == 'organization':
        analysis["insights"].append(f"å›¢é˜Ÿ {entity_a['name']} åœ¨ {entity_b['name']} ä¸­çš„è§’è‰²")
        analysis["insights"].append(f"å›¢é˜Ÿæ–‡åŒ–: {entity_a['culture']}")
        analysis["insights"].append(f"ç»„ç»‡ä½¿å‘½: {entity_b['mission']}")
    
    # ç»„ç»‡ vs ç»„ç»‡
    elif type_a == 'organization' and type_b == 'organization':
        analysis["insights"].append(f"ä¸¤å®¶ç ”ç©¶æœºæ„çš„æ¯”è¾ƒåˆ†æ")
        analysis["insights"].append(f"{entity_a['name']} æ–‡åŒ–: {entity_a['culture']}")
        analysis["insights"].append(f"{entity_b['name']} æ–‡åŒ–: {entity_b['culture']}")
        if entity_a.get('research_philosophy') != entity_b.get('research_philosophy'):
            analysis["insights"].append("ç ”ç©¶å“²å­¦å·®å¼‚: å¯äº§ç”Ÿæ–°çš„ç ”ç©¶èŒƒå¼")
    
    # æ··åˆç±»å‹
    else:
        analysis["insights"].append(f"æ··åˆç±»å‹åˆ†æ: {type_a} + {type_b}")
        analysis["insights"].append("å¯äº§ç”Ÿè·¨å°ºåº¦çš„åˆ›æ–°æ´è§")
    
    return analysis


# ============ v2.0 ä¾¿æ·å‡½æ•° ============

def create_entity(entity_type: str, name: str, **kwargs) -> Dict:
    """åˆ›å»ºå®ä½“å¿«æ·å‡½æ•°"""
    entity = {
        "type": entity_type,
        "name": name,
        **kwargs
    }
    return entity


def run_v2_analysis(
    entities: List[Dict],
    topic: str
) -> Dict:
    """è¿è¡Œ v2.0 åˆ†æ"""
    results = {
        "topic": topic,
        "entities": entities,
        "interactions": [],
        "summary": ""
    }
    
    # åˆ†ææ‰€æœ‰é…å¯¹
    for i in range(len(entities)):
        for j in range(i + 1, len(entities)):
            interaction = analyze_entity_interaction(
                entities[i],
                entities[j],
                topic
            )
            results["interactions"].append(interaction)
    
    # ç”Ÿæˆæ€»ç»“
    entity_types = [e['type'] for e in entities]
    unique_types = set(entity_types)
    
    if len(unique_types) == 1:
        results["summary"] = f"åŒç±»å‹å®ä½“åˆ†æ: {list(unique_types)[0]}"
    else:
        results["summary"] = f"å¤šç±»å‹æ··åˆåˆ†æ: {' + '.join(sorted(unique_types))}"
    
    return results
